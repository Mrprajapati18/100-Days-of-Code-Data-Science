{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6225d649",
   "metadata": {},
   "source": [
    "# Model Evaluation Matrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b328e85",
   "metadata": {},
   "source": [
    " 1. Accuracy:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697e94d0",
   "metadata": {},
   "source": [
    "Accuracy is a fundamental metric for evaluating the performance of a classification model, providing a quick snapshot of how well the model is performing in terms of correct predictions. This is calculated as the ratio of correct predictions to the total number of input Samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a2e874",
   "metadata": {},
   "source": [
    "Accuracy=  Number of correct predictions / Total number of input samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745e7fd6",
   "metadata": {},
   "source": [
    "2. Precision:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9ebd9a",
   "metadata": {},
   "source": [
    "Precision measures the proportion of correctly predicted positive instances out of all instances predicted as positive. It's important when false positives are costly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5dd502",
   "metadata": {},
   "source": [
    "Precision= TP / TP+FP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93f0c43",
   "metadata": {},
   "source": [
    "3. Recall (Sensitivity or True Positive Rate):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80ce18d",
   "metadata": {},
   "source": [
    "Recall measures the proportion of correctly predicted positive instances out of all actual positive instances. It's important when false negatives are costly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6e6064",
   "metadata": {},
   "source": [
    "Recall=  TP/ TP+FN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9289ba",
   "metadata": {},
   "source": [
    "# Area Under Curve(AUC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5da0390",
   "metadata": {},
   "source": [
    "The AUC of a classifier is defined as the probability of a classifier will rank a randomly chosen positive example higher than a negative example. Before going into  AUC  more, let me make you comfortable with a few basic terms. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e1303e",
   "metadata": {},
   "source": [
    "- True positive rate:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a5e921",
   "metadata": {},
   "source": [
    "Also called or termed sensitivity. True Positive Rate is considered as a portion of positive data points that are correctly considered as positive, with respect to all data points that are positive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01898dcd",
   "metadata": {},
   "source": [
    "TPR= TP / TP+FN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998704a4",
   "metadata": {},
   "source": [
    "- True Negative Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc75009",
   "metadata": {},
   "source": [
    "False Negative Rate is considered as a portion of negative data points that are correctly considered as negative, with respect to all data points that are negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf625be",
   "metadata": {},
   "source": [
    "TNR= TN / TN+FP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3213851a",
   "metadata": {},
   "source": [
    "- False-positive Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04ed21e",
   "metadata": {},
   "source": [
    "False Negatives rate is actually the proportion of actual positives that are incorrectly identified as negatives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541e853d",
   "metadata": {},
   "source": [
    "FPR= FP / FP+TN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af7d66a",
   "metadata": {},
   "source": [
    "False Positive Rate and True Positive Rate both have values in the range [0, 1]. Now the thing is what is A U C then? So, A U C  is a curve plotted between False Positive Rate Vs True Positive Rate at all different data points with a range of  [0, 1]. Greater the value of AUCC better the performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3272320",
   "metadata": {},
   "source": [
    "4. F1 Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbe0a97",
   "metadata": {},
   "source": [
    "F1-score is the harmonic mean of precision and recall. It balances precision and recall, making it useful when you want to consider both false positives and false negatives.\n",
    "It is a harmonic mean between recall and precision. Its range is [0,1]. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06723cea",
   "metadata": {},
   "source": [
    "Harmonic mean is one of the Pythagorean means other than Arithmetic Mean and Geometric Mean. The harmonic mean is always lower as compared to the geometric and arithmetic mean. The harmonic mean is calculated by dividing the number of the reciprocal by the sum of the reciprocal values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf0254c",
   "metadata": {},
   "source": [
    "Harmonic Mean (H.M) = n / (1/x1) + (2/ x2) + (1/x3)+ ........+(1/xn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef56a8de",
   "metadata": {},
   "source": [
    "4. Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfedc45",
   "metadata": {},
   "source": [
    "A confusion matrix displays the counts of true positives, true negatives, false positives, and false negatives. It's useful for understanding model performance in classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0945cb0b",
   "metadata": {},
   "source": [
    "- There are 4 terms you should keep in mind: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a68d9fc",
   "metadata": {},
   "source": [
    "1. True Positives: It is the case where we predicted Yes and the real output was also yes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057397e5",
   "metadata": {},
   "source": [
    "2. True Negatives: It is the case where we predicted No and the real output was also No.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8e0c57",
   "metadata": {},
   "source": [
    "3. False Positives: It is the case where we predicted Yes but it was actually No.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdb278e",
   "metadata": {},
   "source": [
    "4. False Negatives: It is the case where we predicted No but it was actually Yes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b10852",
   "metadata": {},
   "source": [
    "# Regression Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43233ca4",
   "metadata": {},
   "source": [
    "- Mean Absolute Error(MAE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406f680d",
   "metadata": {},
   "source": [
    "It is the average distance between Predicted and original values. Basically, it gives how we have predicted from the actual output. However, there is one limitation i.e. it doesnâ€™t give any idea about the direction of the error which is whether we are under-predicting or over-predicting our data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7031d0a1",
   "metadata": {},
   "source": [
    "- Mean Squared Error(MSE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d9efb6",
   "metadata": {},
   "source": [
    "It is similar to mean absolute error but the difference is it takes the square of the average of between predicted and original values. The main advantage to take this metric is here, it is easier to calculate the gradient whereas, in the case of mean absolute error, it takes complicated programming tools to calculate the gradient. By taking the square of errors it pronounces larger errors more than smaller errors, we can focus more on larger errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2da960c",
   "metadata": {},
   "source": [
    "- Root Mean Square Error(RMSE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b007dc",
   "metadata": {},
   "source": [
    "We can say that RMSE is a metric that can be obtained by just taking the square root of the MSE value. As we know that the MSE metrics are not robust to outliers and so are the RMSE values. This gives higher weightage to the large errors in predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
